{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-last, i.e., (None, n_freq, n_time, n_ch)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from builtins import range\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Flatten, Input, Reshape, Dropout, Permute\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # the number of the GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 # percentage to be used\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from global_config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(df_subset, ys, is_shuffle, batch_size=20):\n",
    "    \"\"\"Data generator.\n",
    "    df_subset: pandas dataframe, with rows subset\n",
    "    ys: numpy arrays, N-by-8 one-hot-encoded labels\n",
    "    is_shuffle: shuffle every batch if True.\n",
    "    batch_size: integer, size of batch. len(df_subset) % batch_size should be 0.\n",
    "    \"\"\"\n",
    "    n_data = len(df_subset)\n",
    "    n_batch = n_data // batch_size\n",
    "    if n_data % batch_size != 0:\n",
    "        print(\"= WARNING =\")\n",
    "        print(\"  n_data % batch_size != 0 but this code does not assume it\")\n",
    "        print(\"  so the residual {} sample(s) will be ignored.\".format(n_data % batch_size))\n",
    "\n",
    "    while True:\n",
    "        for batch_i in range(n_batch):\n",
    "            if is_shuffle:\n",
    "                batch_idxs = np.random.choice(n_data, batch_size, replace=False)\n",
    "            else:\n",
    "                batch_idxs = range(batch_i * batch_size, (batch_i + 1) * batch_size)\n",
    "\n",
    "            src_batch = np.array([np.load(os.path.join(DIR_PEDAL_SEGMENT_NPY, df_subset.loc[df_subset.index[i]].filepath.split('.')[0]+'.npy')) for i in batch_idxs],\n",
    "                                 dtype=K.floatx())\n",
    "            src_batch = src_batch[:, np.newaxis, :]  # make (batch, N) to (batch, 1, N) for kapre compatible\n",
    "\n",
    "            y_batch = np.array([ys[i] for i in batch_idxs],\n",
    "                               dtype=K.floatx())\n",
    "            \n",
    "            yield src_batch, y_batch\n",
    "        \n",
    "        \n",
    "def get_callbacks(name,patience):\n",
    "    if not os.path.exists(DIR_SAVE_MODEL):\n",
    "        os.makedirs(DIR_SAVE_MODEL)    \n",
    "    early_stopper = keras.callbacks.EarlyStopping(patience=patience)\n",
    "    model_saver = keras.callbacks.ModelCheckpoint(os.path.join(DIR_SAVE_MODEL,\"{}_best_model.h5\".format(name)),\n",
    "                                                  save_best_only=True)\n",
    "    weight_saver = keras.callbacks.ModelCheckpoint(os.path.join(DIR_SAVE_MODEL,\"{}_best_weights.h5\".format(name)),\n",
    "                                                   save_best_only=True,\n",
    "                                                   save_weights_only=True)\n",
    "    csv_logger = keras.callbacks.CSVLogger(os.path.join(DIR_SAVE_MODEL,\"{}.log\".format(name)))\n",
    "    return [early_stopper, model_saver, weight_saver, csv_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Invariant Models for Pedal-Segment Binary Classification\n",
    "\n",
    "use raw-audio input, which is converted to melspectrogram using Kapre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_multi_kernel_shape(n_out, input_shape=SEGMENT_INPUT_SHAPE,\n",
    "                             out_activation='softmax'):\n",
    "    \"\"\"\n",
    "\n",
    "    Symbolic summary:\n",
    "    > c2' - p2 - c2 - p2 - c2 - p2 - c2 - p3 - d1\n",
    "    where c2' -> multiple kernel shapes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_out: integer, number of output nodes\n",
    "        input_shape: tuple, an input shape, which doesn't include batch-axis.\n",
    "        out_activation: activation function on the output\n",
    "    \"\"\"\n",
    "    audio_input = Input(shape=input_shape)\n",
    "\n",
    "    x = Melspectrogram(n_dft=N_FFT, n_hop=HOP_LENGTH, sr=SR, n_mels=128, power_melgram=2.0, return_decibel_melgram=True)(audio_input)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "\n",
    "    x1 = Conv2D(7, (20, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x2 = Conv2D(7, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x3 = Conv2D(7, (3, 20), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "\n",
    "    x = Concatenate(axis=channel_axis)([x1, x2, x3])\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((4, 4), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    out = Dense(n_out, activation=out_activation, kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "\n",
    "    model = Model(audio_input, out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_crnn_icassp2017_choi(n_out, input_shape=SEGMENT_INPUT_SHAPE,\n",
    "                               out_activation='softmax'):\n",
    "    \"\"\"A simplified model of \n",
    "    Convolutional Recurrent Neural Networks for Music Classification,\n",
    "    K Choi, G Fazekas, M Sandler, K Choi, ICASSP, 2017, New Orleans, USA\n",
    "\n",
    "    Symbolic summary:\n",
    "    > c2 - p2 - c2 - p2 - c2 - p2 - c2 - p2 - r1 - r2 - d1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_out: integer, number of output nodes\n",
    "        input_shape: tuple, an input shape, which doesn't include batch-axis.\n",
    "        out_activation: activation function on the output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    audio_input = Input(shape=input_shape)\n",
    "\n",
    "    x = Melspectrogram(n_dft=N_FFT, n_hop=HOP_LENGTH, sr=SR, n_mels=128, power_melgram=2.0, return_decibel_melgram=True)(audio_input)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(21, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((4, 4), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    if K.image_dim_ordering() == 'channels_first':\n",
    "        x = Permute((3, 1, 2))(x)\n",
    "\n",
    "    x = Reshape((-1, 21))(x)\n",
    "\n",
    "    # GRU block 1, 2, output\n",
    "    x = GRU(41, return_sequences=True, name='gru1')(x)\n",
    "    x = GRU(41, return_sequences=False, name='gru2')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    out = Dense(n_out, activation=out_activation, kernel_regularizer=keras.regularizers.l2(reg_w))(x)\n",
    "\n",
    "    model = Model(audio_input, out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_conv3x3_ismir2016_choi(n_out, input_shape=SEGMENT_INPUT_SHAPE,\n",
    "                                 out_activation='softmax'):\n",
    "    \"\"\" A simplified model of \n",
    "    Automatic Tagging Using Deep Convolutional Neural Networks,\n",
    "    K Choi, G Fazekas, M Sandler, ISMIR, 2016, New York, USA\n",
    "\n",
    "    Symbolic summary:\n",
    "    > c2 - p2 - c2 - p2 - c2 - p2 - c2 - p2 - c2 - p3 - d1\n",
    "\n",
    "    Modifications: \n",
    "        * n_mels (96 -> 32)\n",
    "        * n_channels (many -> [16, 24, 32, 40, 48])\n",
    "        * remove dropout\n",
    "        * maxpooling (irregular to fit the size -> all (2, 2))\n",
    "        * add GlobalAveragePooling2D\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Melspectrogram(n_dft=N_FFT, n_hop=HOP_LENGTH, sr=SR, n_mels=128, power_melgram=2.0, \n",
    "                             return_decibel_melgram=True,\n",
    "                             input_shape=input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "\n",
    "    model.add(Conv2D(10, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(15, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(15, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(20, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(20, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Dense(n_out, activation=out_activation, kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_conv1d_icassp2014_sander(n_out, input_shape=SEGMENT_INPUT_SHAPE,\n",
    "                                   out_activation='softmax'):\n",
    "    \"\"\"A simplified model of\n",
    "    End-to-end learning for music audio,\n",
    "    Sander Dieleman and Benjamin Schrauwen, ICASSP, 2014\n",
    "\n",
    "    Symbolic summary:\n",
    "    > c1 - p1 - c1 - p1 - c1 - p1 - p3 - d1\n",
    "\n",
    "    Modifications: \n",
    "        * Add BatchNormalization\n",
    "        * n_mels (128 -> 32)\n",
    "        * n_layers (2 -> 3)\n",
    "        * add GlobalAveragePooling2D\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_out: integer, number of output nodes\n",
    "        input_shape: tuple, an input shape, which doesn't include batch-axis.\n",
    "        out_activation: activation function on the output\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Melspectrogram(n_dft=N_FFT, n_hop=HOP_LENGTH, sr=SR, n_mels=128, power_melgram=2.0, \n",
    "                             return_decibel_melgram=True,input_shape=input_shape))\n",
    "    model.add(Conv2D(30, (32, 4), padding='valid', kernel_regularizer=keras.regularizers.l2(reg_w)))  # (None, 16, 1, N)\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((1, 4), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(30, (1, 4), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((1, 4), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(30, (1, 4), padding='same', kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((1, 4), padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Dense(n_out, activation=out_activation, kernel_regularizer=keras.regularizers.l2(reg_w)))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Beici: Welcome! Lets do something deep with pedal-segment_npydf_small.csv.\n",
      "       I'm assuming you finished pre-processing.\n",
      "       We're gonna use cnn3x3 model.\n",
      "Beici: Getting model...\n",
      "Beici: Starting to train...\n",
      "Epoch 1/20\n",
      "280/280 [==============================] - 1734s 6s/step - loss: 0.4183 - acc: 0.8046 - val_loss: 0.8242 - val_acc: 0.6091\n",
      "Epoch 2/20\n",
      "280/280 [==============================] - 672s 2s/step - loss: 0.2472 - acc: 0.9031 - val_loss: 1.0469 - val_acc: 0.7047\n",
      "Epoch 3/20\n",
      "280/280 [==============================] - 410s 1s/step - loss: 0.2012 - acc: 0.9241 - val_loss: 0.6449 - val_acc: 0.7965\n",
      "Epoch 4/20\n",
      "280/280 [==============================] - 324s 1s/step - loss: 0.1845 - acc: 0.9310 - val_loss: 1.4936 - val_acc: 0.6906\n",
      "Epoch 5/20\n",
      "280/280 [==============================] - 304s 1s/step - loss: 0.1645 - acc: 0.9406 - val_loss: 0.4349 - val_acc: 0.8681\n",
      "Epoch 6/20\n",
      "280/280 [==============================] - 294s 1s/step - loss: 0.1599 - acc: 0.9429 - val_loss: 0.2416 - val_acc: 0.9185\n",
      "Epoch 7/20\n",
      "280/280 [==============================] - 283s 1s/step - loss: 0.1448 - acc: 0.9495 - val_loss: 0.1907 - val_acc: 0.9368\n",
      "Epoch 8/20\n",
      "280/280 [==============================] - 280s 1s/step - loss: 0.1392 - acc: 0.9518 - val_loss: 0.5709 - val_acc: 0.8409\n",
      "Epoch 9/20\n",
      "280/280 [==============================] - 278s 992ms/step - loss: 0.1341 - acc: 0.9525 - val_loss: 0.2458 - val_acc: 0.9081\n",
      "Epoch 10/20\n",
      "280/280 [==============================] - 279s 996ms/step - loss: 0.1360 - acc: 0.9525 - val_loss: 0.1548 - val_acc: 0.9441\n",
      "Epoch 11/20\n",
      "280/280 [==============================] - 277s 990ms/step - loss: 0.1247 - acc: 0.9571 - val_loss: 0.1316 - val_acc: 0.9517\n",
      "Epoch 12/20\n",
      "280/280 [==============================] - 299s 1s/step - loss: 0.1231 - acc: 0.9574 - val_loss: 0.6516 - val_acc: 0.8200\n",
      "Epoch 13/20\n",
      "280/280 [==============================] - 304s 1s/step - loss: 0.1197 - acc: 0.9587 - val_loss: 0.6621 - val_acc: 0.8246\n",
      "Epoch 14/20\n",
      "280/280 [==============================] - 272s 970ms/step - loss: 0.1215 - acc: 0.9583 - val_loss: 0.1904 - val_acc: 0.9340\n",
      "Epoch 15/20\n",
      "280/280 [==============================] - 276s 985ms/step - loss: 0.1155 - acc: 0.9600 - val_loss: 0.3117 - val_acc: 0.8983\n",
      "Epoch 16/20\n",
      "280/280 [==============================] - 299s 1s/step - loss: 0.1086 - acc: 0.9633 - val_loss: 0.2246 - val_acc: 0.9233\n",
      "Epoch 17/20\n",
      "280/280 [==============================] - 315s 1s/step - loss: 0.1087 - acc: 0.9636 - val_loss: 0.1357 - val_acc: 0.9501\n",
      "Epoch 18/20\n",
      "280/280 [==============================] - 278s 992ms/step - loss: 0.1104 - acc: 0.9628 - val_loss: 0.1299 - val_acc: 0.9519\n",
      "Epoch 19/20\n",
      "280/280 [==============================] - 299s 1s/step - loss: 0.1053 - acc: 0.9648 - val_loss: 0.2767 - val_acc: 0.9045\n",
      "Epoch 20/20\n",
      "280/280 [==============================] - 311s 1s/step - loss: 0.1058 - acc: 0.9647 - val_loss: 0.4314 - val_acc: 0.8703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbbb9e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'pedal-segment_npydf_small.csv'\n",
    "model_name = 'cnn3x3'\n",
    "exp_name = 'small-segment_{}'.format(model_name)\n",
    "reg_w = 1e-4\n",
    "batch_size = 250\n",
    "epochs = 20\n",
    "patience = 10\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Beici: Welcome! Lets do something deep with {}.\".format(dataset_name))\n",
    "print(\"       I'm assuming you finished pre-processing.\")\n",
    "print(\"       We're gonna use {} model.\".format(model_name))\n",
    "csv_path = os.path.join(DIR_PEDAL_METADATA, dataset_name)\n",
    "\n",
    "tracks = pd.read_csv(csv_path)\n",
    "training = tracks.loc[tracks['category'] == 'train']\n",
    "validation = tracks.loc[tracks['category'] == 'valid']\n",
    "test = tracks.loc[tracks['category'] == 'test']\n",
    "\n",
    "# print(\"Beici: We're loading and modifying label values.\")\n",
    "y_train = training.label.values\n",
    "y_valid = validation.label.values\n",
    "y_test = test.label.values\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_valid = keras.utils.to_categorical(y_valid, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "# callbacks\n",
    "callbacks = get_callbacks(name=exp_name, patience=patience)\n",
    "early_stopper, model_saver, weight_saver, csv_logger = callbacks\n",
    "\n",
    "# print(\"Beici: Preparing data generators for training and validation...\")\n",
    "steps_per_epoch = len(y_train) // batch_size\n",
    "gen_train = data_gen(training, y_train, True, batch_size=batch_size)\n",
    "gen_valid = data_gen(validation, y_valid, False, batch_size=batch_size)\n",
    "gen_test = data_gen(test, y_test, False, batch_size=batch_size)\n",
    "\n",
    "print(\"Beici: Getting model...\")\n",
    "if model_name == 'multi_kernel':\n",
    "    model = model_multi_kernel_shape(n_out=2)\n",
    "elif model_name == 'crnn':\n",
    "    model = model_crnn_icassp2017_choi(n_out=2)\n",
    "elif model_name == 'cnn3x3':\n",
    "    model = model_conv3x3_ismir2016_choi(n_out=2)\n",
    "elif model_name == 'cnn1d':\n",
    "    model = model_conv1d_icassp2014_sander(n_out=2)\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "print(\"Beici: Starting to train...\")\n",
    "model.fit_generator(gen_train, steps_per_epoch, epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=gen_valid,\n",
    "                    validation_steps=len(y_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beici: Training is done. Loading the best weights...\n",
      "       Evaluating...\n",
      "Beici: Done for cnn3x3!\n",
      "       test set loss: 0.172541262023\n",
      "       test set accuracy: 0.93580005765\n",
      "       test set auc: 0.99490304\n"
     ]
    }
   ],
   "source": [
    "print(\"Beici: Training is done. Loading the best weights...\")\n",
    "model.load_weights(os.path.join(DIR_SAVE_MODEL,\"{}_best_weights.h5\".format(exp_name)))\n",
    "\n",
    "print(\"       Evaluating...\")\n",
    "scores = model.evaluate_generator(gen_test, len(y_test) // batch_size)\n",
    "y_pred = model.predict_generator(gen_test, len(y_test) // batch_size)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Beici: Done for {}!\".format(model_name))\n",
    "print(\"       test set loss: {}\".format(scores[0]))\n",
    "print(\"       test set accuracy: {}\".format(scores[1]))\n",
    "print(\"       test set auc: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Beici: Welcome! Lets do something deep with pedal-segment_npydf_small.csv.\n",
      "       I'm assuming you finished pre-processing.\n",
      "       We're gonna use multi_kernel model.\n",
      "Beici: Getting model...\n",
      "Beici: Starting to train...\n",
      "Epoch 1/20\n",
      "280/280 [==============================] - 171s 611ms/step - loss: 0.3478 - acc: 0.8543 - val_loss: 0.3815 - val_acc: 0.8123\n",
      "Epoch 2/20\n",
      "280/280 [==============================] - 183s 652ms/step - loss: 0.2211 - acc: 0.9171 - val_loss: 0.1887 - val_acc: 0.9447\n",
      "Epoch 3/20\n",
      "280/280 [==============================] - 172s 613ms/step - loss: 0.1857 - acc: 0.9325 - val_loss: 1.9179 - val_acc: 0.5254\n",
      "Epoch 4/20\n",
      "280/280 [==============================] - 177s 631ms/step - loss: 0.1664 - acc: 0.9409 - val_loss: 0.2856 - val_acc: 0.8755\n",
      "Epoch 5/20\n",
      "280/280 [==============================] - 179s 641ms/step - loss: 0.1485 - acc: 0.9486 - val_loss: 0.2610 - val_acc: 0.8888\n",
      "Epoch 6/20\n",
      "280/280 [==============================] - 183s 655ms/step - loss: 0.1396 - acc: 0.9511 - val_loss: 0.1274 - val_acc: 0.9611\n",
      "Epoch 7/20\n",
      "280/280 [==============================] - 196s 698ms/step - loss: 0.1261 - acc: 0.9580 - val_loss: 0.0863 - val_acc: 0.9735\n",
      "Epoch 8/20\n",
      "280/280 [==============================] - 192s 686ms/step - loss: 0.1170 - acc: 0.9613 - val_loss: 0.0876 - val_acc: 0.9711\n",
      "Epoch 9/20\n",
      "280/280 [==============================] - 200s 714ms/step - loss: 0.1141 - acc: 0.9615 - val_loss: 0.0922 - val_acc: 0.9742\n",
      "Epoch 10/20\n",
      "280/280 [==============================] - 201s 717ms/step - loss: 0.1052 - acc: 0.9650 - val_loss: 0.2892 - val_acc: 0.8696\n",
      "Epoch 11/20\n",
      "280/280 [==============================] - 221s 789ms/step - loss: 0.1014 - acc: 0.9677 - val_loss: 0.2881 - val_acc: 0.8713\n",
      "Epoch 12/20\n",
      "280/280 [==============================] - 187s 669ms/step - loss: 0.0961 - acc: 0.9687 - val_loss: 0.1111 - val_acc: 0.9656\n",
      "Epoch 13/20\n",
      "280/280 [==============================] - 254s 908ms/step - loss: 0.0967 - acc: 0.9686 - val_loss: 0.0720 - val_acc: 0.9794\n",
      "Epoch 14/20\n",
      "280/280 [==============================] - 184s 656ms/step - loss: 0.0905 - acc: 0.9709 - val_loss: 0.2240 - val_acc: 0.9073\n",
      "Epoch 15/20\n",
      "280/280 [==============================] - 193s 691ms/step - loss: 0.0880 - acc: 0.9722 - val_loss: 0.0672 - val_acc: 0.9815\n",
      "Epoch 16/20\n",
      "280/280 [==============================] - 179s 640ms/step - loss: 0.0862 - acc: 0.9726 - val_loss: 0.0761 - val_acc: 0.9765\n",
      "Epoch 17/20\n",
      "280/280 [==============================] - 187s 668ms/step - loss: 0.0821 - acc: 0.9740 - val_loss: 0.0667 - val_acc: 0.9818\n",
      "Epoch 18/20\n",
      "280/280 [==============================] - 178s 635ms/step - loss: 0.0808 - acc: 0.9751 - val_loss: 0.1246 - val_acc: 0.9556\n",
      "Epoch 19/20\n",
      "280/280 [==============================] - 180s 641ms/step - loss: 0.0831 - acc: 0.9730 - val_loss: 0.2238 - val_acc: 0.9075\n",
      "Epoch 20/20\n",
      "280/280 [==============================] - 201s 718ms/step - loss: 0.0769 - acc: 0.9753 - val_loss: 0.1276 - val_acc: 0.9555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66f1128f90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'pedal-segment_npydf_small.csv'\n",
    "model_name = 'multi_kernel'\n",
    "exp_name = 'small-segment_{}'.format(model_name)\n",
    "reg_w = 1e-4\n",
    "batch_size = 250\n",
    "epochs = 20\n",
    "patience = 10\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Beici: Welcome! Lets do something deep with {}.\".format(dataset_name))\n",
    "print(\"       I'm assuming you finished pre-processing.\")\n",
    "print(\"       We're gonna use {} model.\".format(model_name))\n",
    "csv_path = os.path.join(DIR_PEDAL_METADATA, dataset_name)\n",
    "\n",
    "tracks = pd.read_csv(csv_path)\n",
    "training = tracks.loc[tracks['category'] == 'train']\n",
    "validation = tracks.loc[tracks['category'] == 'valid']\n",
    "test = tracks.loc[tracks['category'] == 'test']\n",
    "\n",
    "# print(\"Beici: We're loading and modifying label values.\")\n",
    "y_train = training.label.values\n",
    "y_valid = validation.label.values\n",
    "y_test = test.label.values\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_valid = keras.utils.to_categorical(y_valid, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "# callbacks\n",
    "callbacks = get_callbacks(name=exp_name, patience=patience)\n",
    "early_stopper, model_saver, weight_saver, csv_logger = callbacks\n",
    "\n",
    "# print(\"Beici: Preparing data generators for training and validation...\")\n",
    "steps_per_epoch = len(y_train) // batch_size\n",
    "gen_train = data_gen(training, y_train, True, batch_size=batch_size)\n",
    "gen_valid = data_gen(validation, y_valid, False, batch_size=batch_size)\n",
    "gen_test = data_gen(test, y_test, False, batch_size=batch_size)\n",
    "\n",
    "print(\"Beici: Getting model...\")\n",
    "if model_name == 'multi_kernel':\n",
    "    model = model_multi_kernel_shape(n_out=2)\n",
    "elif model_name == 'crnn':\n",
    "    model = model_crnn_icassp2017_choi(n_out=2)\n",
    "elif model_name == 'cnn3x3':\n",
    "    model = model_conv3x3_ismir2016_choi(n_out=2)\n",
    "elif model_name == 'cnn1d':\n",
    "    model = model_conv1d_icassp2014_sander(n_out=2)\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "print(\"Beici: Starting to train...\")\n",
    "model.fit_generator(gen_train, steps_per_epoch, epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=gen_valid,\n",
    "                    validation_steps=len(y_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beici: Training is done. Loading the best weights...\n",
      "       Evaluating...\n",
      "Beici: Done for multi_kernel!\n",
      "       test set loss: 0.0681605432183\n",
      "       test set accuracy: 0.979400074482\n",
      "       test set auc: 0.99780376\n"
     ]
    }
   ],
   "source": [
    "print(\"Beici: Training is done. Loading the best weights...\")\n",
    "model.load_weights(os.path.join(DIR_SAVE_MODEL,\"{}_best_weights.h5\".format(exp_name)))\n",
    "\n",
    "print(\"       Evaluating...\")\n",
    "scores = model.evaluate_generator(gen_test, len(y_test) // batch_size)\n",
    "y_pred = model.predict_generator(gen_test, len(y_test) // batch_size)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Beici: Done for {}!\".format(model_name))\n",
    "print(\"       test set loss: {}\".format(scores[0]))\n",
    "print(\"       test set accuracy: {}\".format(scores[1]))\n",
    "print(\"       test set auc: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beici: Plot for {}!\".format(model_name))\n",
    "df_log = pd.read_csv(os.path.join(DIR_SAVE_MODEL,\"{}.log\".format(exp_name)))\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "fig.suptitle(\"Accuracy and Loss Plot of Model: {}\".format(model_name))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "tr_acc, = ax1.plot(range(len(df_log)), df_log.acc.values)\n",
    "val_acc, = ax1.plot(range(len(df_log)), df_log.val_acc.values)\n",
    "ax1.legend([tr_acc, val_acc], ['Train Accuracy', 'Val Accuracy'])\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax2 = plt.subplot(122)\n",
    "tr_loss, = ax2.plot(range(len(df_log)), df_log.loss.values)\n",
    "val_loss, = ax2.plot(range(len(df_log)), df_log.val_loss.values)\n",
    "ax2.legend([tr_loss, val_loss], ['Train Loss', 'Val Loss'])\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

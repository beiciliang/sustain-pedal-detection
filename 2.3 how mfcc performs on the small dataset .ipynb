{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from builtins import range\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Flatten, Input, Reshape, Dropout, Permute\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # the number of the GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 # percentage to be used\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from global_config import *\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "N_JOBS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_filepaths(df, dataroot=None):\n",
    "    \"\"\"Generate file path (column name 'filepath') from given dataframe \"\"\"\n",
    "    for filepath in df.filepath.values:\n",
    "        yield os.path.join(dataroot, filepath)\n",
    "        \n",
    "def get_mfcc(filename, dataroot):    \n",
    "    csv_filename = 'pedal-{}_npydf_small.csv'.format(filename)\n",
    "    df = pd.read_csv(os.path.join(DIR_PEDAL_METADATA, csv_filename))\n",
    "    training = df.loc[df['category'] == 'train']\n",
    "    validation = df.loc[df['category'] == 'valid']\n",
    "    print('pedal-{}: Dataframe with size {} for training and {} for validation.'.format(filename,len(training),len(validation)))\n",
    "    \n",
    "    for (task_data, task_name) in zip([training, validation], ['train', 'valid']):\n",
    "        print('Getting MFCC features to {}...'.format(task_name))\n",
    "        npy_filename = 'small-{}_{}_mfcc.npy'.format(filename, task_name)\n",
    "        gen_f = gen_filepaths(task_data, dataroot=dataroot)\n",
    "\n",
    "        pool = Pool(N_JOBS)\n",
    "        paths = list(gen_f)\n",
    "        feats = pool.map(_path_to_mfccs, paths)\n",
    "        feats = np.array(feats)\n",
    "        np.save(os.path.join(DIR_SAVE_MODEL, npy_filename), feats)\n",
    "        print('  done!')\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "def _path_to_mfccs(path):\n",
    "    src = np.load(path)\n",
    "    mfcc = librosa.feature.mfcc(src, SR, n_mfcc=20)\n",
    "    dmfcc = mfcc[:, 1:] - mfcc[:, :-1]\n",
    "    ddmfcc = dmfcc[:, 1:] - dmfcc[:, :-1]\n",
    "    return np.concatenate((np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
    "                           np.mean(dmfcc, axis=1), np.std(dmfcc, axis=1),\n",
    "                           np.mean(ddmfcc, axis=1), np.std(ddmfcc, axis=1)), axis=0)\n",
    "\n",
    "def load_xy(filename, task_name):\n",
    "    \n",
    "    npy_filename = 'small-{}_{}_mfcc.npy'.format(filename, task_name)\n",
    "    x = np.load(os.path.join(DIR_SAVE_MODEL, npy_filename))\n",
    "    \n",
    "    csv_filename = 'pedal-{}_npydf_small.csv'.format(filename)\n",
    "    df = pd.read_csv(os.path.join(DIR_PEDAL_METADATA, csv_filename))\n",
    "    task_data = df.loc[df['category'] == task_name]   \n",
    "    y = task_data.label.values\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MFCC features from the small datasets\n",
    "\n",
    "Features are saved in `./save-model/small-{onset or segment}_{train or valid}_mfcc.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = ['onset', 'segment']\n",
    "# dataroots = [DIR_PEDAL_ONSET_NPY, DIR_PEDAL_SEGMENT_NPY]\n",
    "# for idx, (filename, dr) in enumerate(zip(filenames, dataroots)):\n",
    "#     get_mfcc(filename, dataroot=dr)\n",
    "\n",
    "# '''\n",
    "# pedal-onset: Dataframe with size 70000 for training and 20000 for validation.\n",
    "# Getting MFCC features to train...\n",
    "#   done!\n",
    "# Getting MFCC features to valid...\n",
    "#   done!\n",
    "# pedal-segment: Dataframe with size 70000 for training and 20000 for validation.\n",
    "# Getting MFCC features to train...\n",
    "#   done!\n",
    "# Getting MFCC features to valid...\n",
    "#   done!\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do grid search cross validation to get the best SVM parameters for binary classification\n",
    "\n",
    "Run `python mfcc_gridsearch.py` to save the best parameters in `./save-model/small-{onset or segment}_mfcc_svc_best_params.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny version of mfcc_gridsearch.py using a small portion of data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "class OptionalStandardScaler(StandardScaler):\n",
    "    def __init__(self, on=False):\n",
    "        self.on = on  # bool\n",
    "        if self.on:\n",
    "            super(OptionalStandardScaler, self).__init__(with_mean=True, with_std=True)\n",
    "        else:\n",
    "            super(OptionalStandardScaler, self).__init__(with_mean=False, with_std=False)\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "n_jobs = int(n_cpu * 0.8)\n",
    "print('There are {} cpu available, {} (80%) of them will be used for our jobs.'.format(n_cpu, n_jobs))\n",
    "\n",
    "# gps = [{\"C\": [0.1, 2.0, 8.0, 32.0], \"kernel\": ['rbf'],\n",
    "#         \"gamma\": [0.5 ** i for i in [3, 5, 7, 9, 11, 13]] + ['auto']},\n",
    "#        {\"C\": [0.1, 2.0, 8.0, 32.0], \"kernel\": ['linear']}\n",
    "#       ]\n",
    "gps = [{\"C\": [0.1, 2.0], \"kernel\": ['rbf'],\n",
    "        \"gamma\": [0.5 ** i for i in [3, 5]]},\n",
    "       {\"C\": [0.1, 2.0], \"kernel\": ['linear']}\n",
    "      ]\n",
    "classifier = SVC\n",
    "dataroots = [DIR_PEDAL_ONSET_NPY, DIR_PEDAL_SEGMENT_NPY]\n",
    "filenames = ['onset', 'segment']\n",
    "\n",
    "for filename in filenames:\n",
    "        \n",
    "    x_train, y_train = load_xy(filename, task_name='train')\n",
    "    x_valid, y_valid = load_xy(filename, task_name='valid')\n",
    "    # x = np.concatenate((x_train, x_valid), axis=0)\n",
    "    # y = np.concatenate((y_train, y_valid), axis=0)\n",
    "    # cv = [(np.arange(len(x_train)), np.arange(len(x_train),len(x)))]\n",
    "    x = np.concatenate((x_train[:30], x_valid[:10]), axis=0)\n",
    "    y = np.concatenate((y_train[:30], y_valid[:10]), axis=0)\n",
    "    cv = [(np.arange(30), np.arange(30,len(x)))]\n",
    "    clname = classifier.__name__\n",
    "    estimators = [('stdd', OptionalStandardScaler()), ('clf', classifier())]\n",
    "    pipe = Pipeline(estimators)\n",
    "\n",
    "    params = []\n",
    "    for dct in gps:\n",
    "        sub_params = {'stdd__on': [True, False]}\n",
    "        sub_params.update({'clf__' + key: value for (key, value) in dct.iteritems()})\n",
    "        params.append(sub_params)\n",
    "\n",
    "    clf = GridSearchCV(pipe, params, cv=cv, n_jobs=n_jobs, pre_dispatch='8*n_jobs').fit(x, y)\n",
    "    save_npy_path = os.path.join(DIR_SAVE_MODEL,'testmfcc-{}.npy'.format(filename))\n",
    "    np.save(save_npy_path, [clf.best_params_])\n",
    "    print('best score of pedal-{} {}: {}'.format(filename, clname, clf.best_score_))\n",
    "    print(clf.best_params_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the SVM with best parameters to get the scores on the small validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "filenames = ['onset', 'segment']\n",
    "dataroots = [DIR_PEDAL_ONSET_NPY, DIR_PEDAL_SEGMENT_NPY]\n",
    "\n",
    "for filename in filenames:\n",
    "    print('===== Pedal-{} SVC Best Parameters ====='.format(filename))\n",
    "    x_train, y_train = load_xy(filename, task_name='train')\n",
    "    x_valid, y_valid = load_xy(filename, task_name='valid')\n",
    "        \n",
    "    save_npy_path = os.path.join(DIR_SAVE_MODEL,'small-{}_mfcc_svc_best_params.npy'.format(filename))\n",
    "    parameter = np.load(save_npy_path)[0]\n",
    "    if parameter['clf__kernel']=='linear':\n",
    "        clf = SVC(kernel='linear', C=parameter['clf__C']).fit(x_train, y_train)\n",
    "    else:\n",
    "        clf = SVC(kernel=parameter['clf__kernel'], C=parameter['clf__C'], gamma=parameter['clf__gamma']).fit(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_valid)\n",
    "    loss = log_loss(y_valid, y_pred)\n",
    "    acc = clf.score(x_valid, y_valid)\n",
    "    auc = roc_auc_score(y_valid, y_pred)\n",
    "    \n",
    "    print('{}'.format(parameter))\n",
    "    print(\"      valid set loss: {}\".format(loss))\n",
    "    print(\"  valid set accuracy: {}\".format(acc))\n",
    "    print(\"       valid set auc: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
